Токены — это фундаментальные единицы текста, обрабатываемые LLM. Они создаются путём разбиения текста на более мелкие компоненты, такие как слова, подслова или символы. Понимание токенов критически важно, поскольку модели предсказывают следующий токен в последовательностях, стоимость API зависит от количества токенов, а модели имеют ограничения по максимальному количеству токенов на входе и выходе.