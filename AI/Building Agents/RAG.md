**RAG (Retrieval-Augmented Generation)** — это передовая архитектура в искусственном интеллекте, которая сочетает в себе **поиск** информации из внешних источников (баз знаний) и **генерацию** ответов.

Проще говоря, это технология, которая позволяет таким моделям, как ChatGPT, **"заглядывать в свои notes" (базу знаний)** перед тем, как ответить на вопрос. Это делает ответы более точными, актуальными и лишёнми "галлюцинаций".

Процесс RAG можно представить в виде трёх этапов:

**1. Retrieval (Поиск)**

- Когда пользователь задаёт вопрос (запрос), система не отправляет его сразу в LLM.
    
- Вместо этого она сначала ищет **наиболее релевантные документы** из заранее подготовленной базы знаний. Эта база может содержать PDF-файлы, статьи, документацию, данные с сайтов и т.д.
    
- Поиск происходит с помощью **векторного поиска**. И запрос пользователя, и все документы в базе преобразуются в **эмбеддинги** (числовые векторы). Система находит документы, векторы которых ближе всего к вектору запроса.
    

**2. Augmentation (Обогащение)**

- Найденные релевантные фрагменты текста (например, 2-3 самых подходящих абзаца) **добавляются к оригинальному запросу пользователя**.
    
- Формируется новый, расширенный (обогащённый) prompt (запрос к модели), который выглядит примерно так:
    
    > "Используй строго следующую информацию из нашей базы знаний:  
    > [Здесь вставляется найденный релевантный текст]
    > 
    > Ответь на вопрос пользователя на основе ЭТОЙ информации:  
    > Вопрос: [Оригинальный вопрос пользователя]"
    

**3. Generation (Генерация)**

- Этот обогащённый prompt отправляется в большую языковую модель (например, GPT-4).
    
- Модель не полагается на свою память, а **генерирует ответ, основываясь исключительно на предоставленных ей документах**.
    
- В результате пользователь получает точный, обоснованный ответ с ссылкой на источник (если система это предусматривает).


## **How RAG works**
1. Что такое Retrieval-Augmented Generation (RAG)  
    Без RAG большая языковая модель (LLM) отвечает только на основе того, что она выучила при обучении. Это «замороженные» знания до момента окончания тренировки.  
    С RAG добавляется компонент извлечения информации:

- Берётся пользовательский запрос.
- Система сначала ищет по внешним источникам данных то, что может быть релевантно.
- Затем в модель подаётся и сам запрос, и найденные фрагменты.  
    Итог: ответ становится точнее и актуальнее, потому что модель опирается не только на свои базовые знания, но и на свежие данные.

2. Внешние данные (External Data)  
    Это любые данные, которых изначально нет внутри модели:

- Базы данных (SQL/NoSQL), файловые хранилища, wiki, CRM, API, документы (PDF, DOCX), внутренние регламенты и т.п.  
    Чтобы модель могла «понимать» эти данные, их переводят в числовое представление — эмбеддинги (векторные представления текста).
- Специальная модель эмбеддингов превращает текст в векторы (набор чисел).
- Эти векторы сохраняются в векторной базе (vector store / vector database).  
    Это формирует своеобразную «библиотеку знаний», пригодную для быстрого семантического поиска.

3. Извлечение (Retrieve) релевантной информации  
    Когда пользователь задаёт вопрос:

- Его запрос тоже переводится в вектор.
- Выполняется поиск ближайших (по косинусной близости или другому метрике) векторов в базе.  
    Пример: сотрудник спрашивает: «Сколько у меня ежегодного отпуска?»  
    Система может извлечь:
- Политику компании по отпуску.
- Личные данные сотрудника о прошлых отпусках.  
    То есть возвращаются документы, максимально «семантически близкие» запросу.

4. Дополнение запроса (Augment)  
    На шаге генерации формируется расширенный промпт:

- Берётся исходный вопрос.
- К нему добавляются извлечённые отрывки (контекст).  
    С помощью приёмов prompt engineering это упаковывается в структуру (например: «Контекст: … Вопрос: … Ответь строго по контексту»).  
    Это снижает галлюцинации модели и повышает конкретику.

5. Обновление внешних данных  
    Проблема: данные устаревают (новые регламенты, обновлённые цены и т.д.).  
    Нужно периодически:

- Находить изменившиеся документы.
- Перегенерировать эмбеддинги (переиндексация).  
    Это можно делать:
- В реальном времени (триггеры, обновления по событию).
- Пакетно (каждый час/день/неделю).  
    Главная цель — актуальность результатов поиска.

6. Визуальная схема (в тексте упомянута) обычно отражает цепочку:  
    Пользовательский запрос -> Преобразование в вектор -> Поиск релевантных фрагментов -> Формирование расширенного промпта -> Генерация ответа LLM -> (Опционально) кэширование / логирование.
    
7. Чем RAG отличается от просто семантического поиска  
    Семантический поиск сам по себе:
    

- Находит релевантные фрагменты и может возвращать их как результат.  
    RAG:
- Берёт эти фрагменты и использует их внутри LLM, чтобы построить связный, целостный ответ на естественном языке.

8. Роль семантического поиска внутри RAG  
    Семантический поиск — это улучшенный механизм извлечения, который:

- Понимает смысл запроса, а не лишь точные ключевые слова.
- Находит более точные отрывки среди огромного массива корпоративных знаний (мануалы, FAQ, отчёты, инструкции, HR-документы).
- Может возвращать не просто документ целиком, а релевантные фрагменты (passages, chunks) упорядоченные по значимости.

9. Ограничения классического (ключевого) поиска  
    Ключевое (keyword) индексирование:

- Зависит от точного совпадения слов.
- Пропускает семантически близкие формулировки («больничный» vs «лист нетрудоспособности»).
- Требует ручной подготовки данных: разбиение на куски (chunking), очистка, нормализация.  
    Семантический подход:
- Автоматически создаёт эмбеддинги.
- Лучшая устойчивость к перефразированию.
- Позволяет ранжировать по смысловой близости.

10. Почему это повышает качество RAG  
    Если просто взять «сырые» документы, модель может:

- Перегрузиться лишним контекстом.
- Сгенерировать неточный ответ.  
    Семантический поиск возвращает короткие, релевантные фрагменты → промпт компактный → больше токенов остаётся на генерацию → меньше шума → лучше фактическая точность.

11. Ключевые термины простыми словами

- Эмбеддинг([[Embeddings]]): «Отпечаток смысла» текста в виде чисел.
- Векторная база: место, где хранятся эти отпечатки, чтобы быстро находить похожие.
- [[Chucking]]: разрезание больших документов на фрагменты (например, по 500–1000 токенов) для точного извлечения.
- RAG: «Схема: найди — подставь — сгенерируй».
- Семантический поиск: «Пойми смысл и найди близкие по смыслу куски текста».

12. Итоговое упрощённое сравнение  
    Без RAG: Модель «думает» только памятью обучения.  
    RAG + keyword search: Модель получает контекст, но он может быть не самым точным.  
    RAG + semantic search: Модель получает максимально подходящий по смыслу контекст и отвечает качественнее.

Короткое резюме  
RAG — это способ «вживлять» в LLM актуальные внешние знания через этап извлечения и дополнения промпта. Семантический поиск — критический механизм внутри этой схемы, который делает извлечение точным, а итоги генерации — более релевантными и полезными. Разница: семантический поиск занимается поиском смысла, а RAG — полной цепочкой: поиск + генерация ответа с использованием найденного.

Если нужно: могу привести архитектурный пример или подсказать, как реализовать минимальный прототип (ингест данных -> эмбеддинги -> vector store -> retrieval -> prompt assembly -> LLM). Просто скажите.